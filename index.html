<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="未实践之事，无评价之理。">
<meta property="og:type" content="website">
<meta property="og:title" content="VKYH-2017-8-17">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="VKYH-2017-8-17">
<meta property="og:description" content="未实践之事，无评价之理。">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VKYH-2017-8-17">
<meta name="twitter:description" content="未实践之事，无评价之理。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>VKYH-2017-8-17</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">VKYH-2017-8-17</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">未实践之事，无评价之理。</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/多标签分类场景处理思路/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/05/多标签分类场景处理思路/" itemprop="url">多标签分类场景处理思路</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-05T16:40:33+08:00">
                2020-03-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文关键字： <strong>Hexo绿色版</strong>，<strong>Hexo便携版</strong>，<strong>Hexo配置</strong>，<strong>Hexo</strong>，<strong>U盘</strong><br><strong>1、多标签分类场景定义</strong></p>
<p>常规的文本分类场景主要为单标签多类别分类问题。而对于描述用户画像之类的场景而言，仅仅用单个标签词将不足以准确描述用户特征情况。现实中将从几个不同角度进行用户画像描述，这也就引出本文模型场景——多标签分类(Muti-label)问题。</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/1.png?raw=true" alt="图1 多标签分类场景"></p>
<p>多标签分类问题（Muti-label Task）如上图所示。假设输入数据集集合为X={X1,X2,X3,X4,X5}，X集合中每一个元素Xi表示一个句向量。而对应的集合Y={Y1,Y2,Y3,Y4,Y5}经过独热编码处理，即表示从5个不同角度描述各个句子的标签类别。而与之难以区分的场景为多类别标签分类问题(Muti-class)，具体场景如下图所示。</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/2.png?raw=true" alt="图2 多类别分类场景"></p>
<p>在相应多类别分类数据中，每一个数据实例对应的分类标签为确定类型标签，但不同类别的标签同属一种角度描述实例的标签类别。对于多标签分类问题而言，不同角度的标签集合Y={Y1,Y2,Y3,Y4,Y5}内部存在相互独立和相互关联的两种情况。不同情况模型构建思路也将不同，具体将在下节进行讲解。</p>
<p><strong>2、多标签分类场景模型构建方案</strong></p>
<p><strong><strong>2.1 标签角度相互独立的情况</strong></strong></p>
<p><strong><strong>2.1.1 转化为二分类方法</strong></strong></p>
<p>对于多标签分类问题，若标签相互独立，且不相关，则第一种处理思路如下图所示。将各角度标签作为不同数据实例的分类标签，通过单独构建二分类模型进行建模处理。按照此方法，若存在K个角度描述的标签，则需要构建K个二分类模型。对于每一个二分类模型而言，其输出类别为“是/否符合此类标签”。该方法对于研究领域而言，建模思路简单，模型效果较好。但工程领域而言，该方案需要重复性模型构建过程，模型数量过多的情况下将不利于模型部署工作。鉴于此，此类方法并非多标签分类场景下的最优解。</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/3.png?raw=true" alt="图3 多个二分类场景"></p>
<p><strong><strong>2.1.2 转化为多分类方法</strong></strong></p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/4.png?raw=true" alt="图4 多分类场景"></p>
<p>如上图所示，对于多标签分类场景而言，其特点在于标签角度繁多，若能以一种方式将分类方式构建到一个模型中进行处理，则将大大简化具体分类计算复杂度。在转化为多分类策略中，其通过标签合并做法，将完整的Y={Y1,Y2,Y3,Y4,Y5}看做一个大类的合并标签集合。在模型分类器过程中，仅仅只需要进行一次softmax运算，即可转化为相应标签类别。该方法相对于K个二分类而言效率较高，但其问题在于标签合并过程中，无法确保各合并标签数据集分布特性。很可能出现合并标签中，某些类别数量较少，某些类别数量较多，从而导致模型构建过程中存在大量训练偏差。该方法应在标签合并过程中，通过欠采样的数据处理方式，以确保各类别数据分布比例适中为核心目标。最后，对于各类别比例适中的数据集便可使用此方法。但该方法页不适用于多标签存在关联的场景，即在多标签集合中，内部各标签Y互换位置也可能存在整体标签合并集合不一致的情况。</p>
<p><strong><strong>2.2 标签角度相互关联的情况</strong></strong></p>
<p>对于类别标签集合Y={Y1,Y2,Y3,Y4,Y5}而言，其内部标签若存在相互关联情况，从而导致传统分类方法无法应对此类需求。目前常见做法，是通过构建大规模标签类别词典进行相关词检索，同时辅以词相似度计算过程，倘若在目标文本中找到了相应标签词或者相似词组，则该文本中即包含相应标签。该方法需要投入大量精力进行语料库归纳，检索，不够智能。<br>对此场景而言，还有一种思路是转化为序列生成场景，但相关过程将在详细讲解了seq模型之后进行补充。</p>
<p><strong><strong>2.3 通用建模方法</strong></strong></p>
<p>那么，是否存在一种通用情况呢？目前而言还真有，可以将其称为组合二分类模型。具体建模思路如下。组合二分类模型结构如下图所示，将输入分类器的（batch，maxlen，d）的文本特征张量进行一系列维度变化，最终生成形状为（batch，Y）的分类结果矩阵。该矩阵长度batch表示句子实例，Y表示每个句子的多个角度的标签合并集合。若标签YI产生概率大于等于sigmoid函数临界条件，则该句子实例对应的该角度标签为1，反之为0. 核心通过设置sigmoid激活函数。该方法的核心在于通过矩阵运算，同时结合sigmoid激活函数。将多个角度的标签输入到同一个模型中，通过最后输出各类别标签状态矩阵，进行组合二分类处理，从而得到句子各类别的分类结果。</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/5.png?raw=true" alt="图5 组合二分类模型"></p>
<p>对于组合二分类过程，本文将进行如下矩阵变化实验进行流程演示：</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/6.png?raw=true" alt="图6 模拟分类器输入矩阵"></p>
<blockquote>
<ul>
<li>1、    假设模型输入到分类器的矩阵如上图所示，该矩阵形状为（6,10），而对应的类别集合为Y={Y1,Y2,Y3,Y4,Y5}，其中宽度10表示文本信息展开量，长度6表示句子实例个数。对于矩阵而言，每一行即表示相应句子的句向量。</li>
</ul>
</blockquote>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/7.png?raw=true" alt="图7 分类结果矩阵"></p>
<blockquote>
<ul>
<li>2、    其次将其输入到线性层中，通过矩阵运算改变矩阵维度。后续经过sigmoid激活函数处理。最终得到分类结果矩阵如上图所示。对于分类结果矩阵而言，其长度为6，表示句子数目，宽度为5表示对应的标签类。矩阵内部元素表示对于每一个句子而言，能被归类到某一类标签的概率大小。</li>
</ul>
</blockquote>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/8.png?raw=true" alt="图8 标签类别矩阵"></p>
<blockquote>
<ul>
<li>3、后续对于分类结果矩阵，通过选取sigmoid激活函数阈值0.5，来进行类别转换。凡是概率大于等于0.5的分类元素，其对应的标签类别矩阵值为1，反之为0。最终生成如上图所示的标签类别矩阵。图中该矩阵形状为（6 , 5），即表示每一个句子能被归为的标签类别情况。</li>
</ul>
</blockquote>
<p>综上即为组合二分类模型流程完整流程，各位朋友可以自行尝试下。最后，torch技术非常符合自身需求，后期相关实践将无缝对接该技术。</p>
<p>最后，原创不易，若有空请博主喝杯咖啡，将会大大激励更多博文产出哟！二维码如下，hhh：</p>
<p><img src="https://github.com/yanhan19940405/imgclub/blob/master/9.jpg?raw=true" alt="图9 二维码"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/07/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/07/hello-world/" itemprop="url">Hello World</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-07T16:49:28+08:00">
                2020-01-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop.png" alt=""></p>
<blockquote>
<p><strong>本版本仅适用于Win环境</strong><br>本文关键字： <strong>Hexo绿色版</strong>，<strong>Hexo便携版</strong>，<strong>Hexo配置</strong>，<strong>Hexo</strong>，<strong>U盘</strong></p>
</blockquote>
<p><a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>是一个快速、简洁且高效的博客框架，支持 GitHub Flavored Markdown 的所有功能；具有超快生成速度，让上百个页面在几秒内瞬间完成渲染；还拥有各式各样的插件等等。</p>
<p>但是就像很多教程里面写的那样，搭建 Hexo 本地环境，需要安装 Node.js、Git 以及使用 npm 进行安装和配置。这对于毫无经验的新手来说，是一个很大的挑战。同时，由于这些环境的存在，导致如果需要更换计算机的时候，重新安装配置一个新的Hexo环境，又得花费一些功夫。</p>
<p>所以呢，锵锵，我们整合了一个 Hexo 便携版，来简化本地环境的部署。</p>
<p>####<strong>版本介绍</strong><br>那么所谓的便携版到底是什么？便携版就是将 Hexo 本地环境所需要的各种依赖环境的整合到一起，做成的不需要安装的版本。</p>
<p>本便携版（Release 1.0.0）所包含的软件如下：</p>
<blockquote>
<ul>
<li>Git: 2.7.4</li>
<li>Nodejs: 6.10.1</li>
<li>Npm: 4.4.1</li>
<li>Hexo: 3.2.2</li>
</ul>
</blockquote>
<p>为了便携的需要，不能配置固定的环境变量，所以除此之外还有相应的批处理文件，下文将详细介绍。</p>
<p>####<strong>从零开始，1分钟搭建Hexo写作环境</strong><br>说了这么多，我们这就开始教你如何在1分钟内，从零开始搭建Hexo写作环境！</p>
<p>#####1 注册一个Github帐号</p>
<p>1.1 进入<a href="https://github.com" target="_blank" rel="external">Github</a>，并在右边的3个框框中分别填写 用户名、邮箱地址、账户密码，并点击 <strong><code>Sign up for Github</code></strong>;</p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop2.png" alt="图1.1.1"></p>
<p>1.2 进入欢迎页面后，点击<strong><code>Finish sign up</code></strong>;</p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop3.png" alt="图1.2.1"></p>
<p>1.3 进入到这一步之后（<strong>图1.3.1</strong>），先别记着点任何东西，查看你的邮箱，应该会收到如下的邮件（<strong>图1.3.2</strong>），确认你的邮件。否则，你会看到错误页面（<strong>图1.3.3</strong>）。query完毕之后在<strong>图1.3.1</strong>所示的页面，点击<strong><code>+ New repository</code></strong>;</p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop5.png" alt="图1.3.1"></p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop6.png" alt="图1.3.2"></p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop7.png" alt="图1.3.3"></p>
<p>1.4 请在红框中的 <strong><code>Repository name</code></strong> 里面输入 <strong><code>用户名.github.io</code></strong> ( <strong>用户名</strong> 就是你刚刚 <strong>步骤1.1</strong> 中注册的用户名)，然后点击 <strong><code>Create repository</code></strong>;</p>
<p><img src="https://portablehexo.bitmoe.cn/hexopp/hexop8.png" alt="图1.4.1"></p>
<p>1.5 至此，Github网页注册的部分完毕</p>
<p>####2 下载HEXO Portable</p>
<p>2.1 访问我们的 <a href="https://project.bitmoe.cn/PortableHexo/" target="_blank" rel="external">HEXO Portable</a> 页面，或者 <a href="https://github.com/Bitmoe/PortableHexo" target="_blank" rel="external">Github仓库</a> 下载最新的便携版；</p>
<p>2.2 双击得到的自解压文件，解压到你需要安装博客的地方，可以选择硬盘或者U盘；</p>
<p>####3 配置你的环境</p>
<p>3.1 在你的博客的文件夹，你会发现很多批处理文件，我先介绍下各个文件的用途：</p>
<blockquote>
<ul>
<li>配置基本信息 &gt;&gt; 配置博客的基本环境</li>
<li>配置Github部署 &gt;&gt; 配置博客的部署</li>
<li>启动命令行 &gt;&gt; 启动带环境变量的Git-Bash，否则无法使用node、npm、git等命令</li>
<li>新建文章 &gt;&gt; 新建一篇文章</li>
<li>渲染并本地测试 &gt;&gt; 生成并本地预览</li>
<li>渲染并部署 &gt;&gt; 生成并部署到GitHub</li>
<li>重置配置文件 &gt;&gt; 重置_config.yml</li>
</ul>
</blockquote>
<p>3.2 我们继续进行环境配置，双击<strong><code>配置基本信息</code></strong>并按顺序输入以下信息：</p>
<blockquote>
<ul>
<li>主标题：顾名思义</li>
<li>副标题：顾名思义</li>
<li>描述：顾名思义</li>
<li>作者：顾名思义</li>
<li>网站地址：填写 <strong>用户名.github.io</strong> （此处的用户名为<strong>步骤1.1</strong>中填写的用户名）<br><strong>如果您已经购买了域名，可以参看相关的文章！</strong></li>
</ul>
</blockquote>
<p>3.3 双击<strong><code>配置Github部署</code></strong>并按顺序输入以下信息：</p>
<blockquote>
<ul>
<li>Github的用户名：此处的用户名为<strong>步骤1.1</strong>中填写的用户名</li>
<li>Github注册邮箱：此处的注册邮箱为<strong>步骤1.1</strong>中填写的注册邮箱</li>
</ul>
</blockquote>
<p>3.4 输入完毕之后，会进行测试部署，等待屏幕提示<strong>下面将进行部署测试，稍后将有一个openssh的对话框出现，请输入你的github用户密码。</strong>的时候，按回车继续；（此处的用户密码为<strong>步骤1.1</strong>中填写的用户密码）</p>
<p>3.5 然后继续等待，此时屏幕会出现n多行，耐心等待即可；</p>
<p>3.6 等待屏幕提示<strong>请访问 <a href="https://用户名.github.io" target="_blank" rel="external">https://用户名.github.io</a> 查看是否部署成功！</strong>的时候，在浏览器中输入<strong><a href="https://用户名.github.io" target="_blank" rel="external">https://用户名.github.io</a> </strong>，理论上可以看到你的Hexo站点！（此处的用户名为<strong>步骤1.1</strong>中填写的用户名）</p>
<p>3.7 至此，你的Hexo环境就配置完成了，这样你的这个Hexo博客文件夹无论移动到哪一台电脑，都能通过<strong>启动命令行</strong>出现的bash命令行，或者其余便捷的批处理文件比如<strong>新建文章</strong>、<strong>渲染并本地测试</strong>、<strong>渲染并部署</strong>进行相应的操作。</p>
<p>3.8 如果你在配置中出错，可以运行<strong>重置配置文件</strong>后，再重复<strong>3.1~3.6步骤</strong></p>
<p>####4 开始享受纯粹的Hexo写作吧！</p>
<p>####5 备注</p>
<blockquote>
<ul>
<li>本便携版以及本文均使用 <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="external">CC BY-NC-SA 4.0</a>协议；</li>
<li>本文所有权归 <a href="http://qistchan.com" target="_blank" rel="external">QistChan</a> &amp; <a href="https://wwww.bitmoe.com" target="_blank" rel="external">Bitmoe Inc.</a> 所有；</li>
<li>本便携版由 <a href="https://www.bitmoe.com" target="_blank" rel="external">Bitmoe Inc.</a> 维护并提供技术支持；</li>
<li>需要技术支持可以在<a href="https://github.com/Bitmoe/PortableHexo" target="_blank" rel="external">Github仓库</a> 提交<strong><code>Issues</code></strong>；</li>
<li>Written with <a href="https://stackedit.io/" target="_blank" rel="external">StackEdit</a>.</li>
</ul>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/27/Transformer模型及基于Keras的复现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/27/Transformer模型及基于Keras的复现/" itemprop="url">Transformer模型及基于Keras的复现</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-27T17:23:12+08:00">
                2019-03-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>1.动态词向量技术</strong><br>先说以下动态词向量吧。这个东西其实本质上来讲还是需要由word embedding基础上进一步处理。其借助于seq2seq模型，经过encoder端将文本序列动态编码成数值矩阵，其本质上已经脱离了传统词向量模型基于词相似度生成的特征矩阵。对于动态这种词的理解，可以按照Transformer来理解。在encoder端中，通过几层self-attention并行处理，其依据序列语言模型结构，动态计算每一个词的att权值，从而给了每个词的分布概率。最终依托此权值进一步生成attention数值编码。这种迁移学习的编码方式得到的矩阵中包含了文本上下文信息，也解决了部分语序理解问题。</p>
<p>而传统的词向量模型，只是单纯依托于词相似度，同时通过建立的索引词典进行数值编码，这种方式无法解决同义词问题。对于文本前后信息的理解也并未达到良好程度，因此算是落后了吧。<br><strong>2.Transformer模型理论与结构</strong><br><img src="https://upload-images.jianshu.io/upload_images/10738320-545df756d1e3867e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 Transformer图"></p>
<p>Transformer模型结构其实很简单，其本质上是seq2seq模型的变种。首先在Encoder端，其通过将输入文本经过embedding层look up操作后，得到Word embedding矩阵。然后与词位置矩阵进行加性连接，赋予静态词向量词位置关系这类特征。在此基础上输入到多层self attention层构成的Muti-head注意力层中建立序列的动态编码矩阵，后为了避免深度网络所带来的梯度消失问题，做了一次残差连接。通过一层BP神经网络与第二次残差，即得到了最终encoder端的编码值。而在Decoder端，将target序列进行同样嵌入操作后，通过一次带有mask操作的muti-attention层处理，结合encoder端生成的编码矩阵（也叫state状态矩阵）再次经过attention+BP网络处理。最终通过一步线性变化，通过softmax即可得到序列中各个词的输出概率大小。在模型构建中，还加入了Batch Normalization层加速训练，同时避免过拟合。<br><strong>3.Transformer模型的应用</strong><br>用法很简单，首先在分类场景下，你只需要一个encoder+softmax即可作为文本分类的baseline，后续进行微调即可。当然，如果你觉得只有encoder不舒服，在后面自己接入其他神经网络模型进行处理也可以。其次，对于文本摘要，NMT问题，生成式对话而言，Decoder端是需要的，但不一定和论文中decoder结构一致，此处不再扩展声明。从这个角度上来看，Transformer反而Encoder是最重要的结构了。<br><strong>4.基于keras的模型复现</strong><br>这部分请<a href="https://github.com/yanhan19940405/Transformer" target="_blank" rel="external">跳转至GIT仓库查看吧</a>，欢迎提Issue。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/自然语言处理语序问题及解决思路（部分附源码实现）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/24/自然语言处理语序问题及解决思路（部分附源码实现）/" itemprop="url">自然语言处理语序问题及解决思路（部分附源码实现）</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-24T03:00:49+08:00">
                2019-03-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在自然语言领域，语序问题对于文本特征表述，生成序列等场景都有很强的影响份量。本文将专门针对此场景进行理论汇总。同时将给出解决方案。</p>
<p><strong>一．语序对于文本特征提取的影响与优化</strong><br>在自然语言领域，对于文本特征提取方式，从当初基于词频构建的矩阵，由于缺乏对于同义词辨析与文本上下文信息的解读，使得模型训练依然存在较大误差。到后来基于词相似度所构成的Word2vec词向量方案及后续变种出现后，解决了部分文本上下文信息问题，但依然存在词义辨析问题。鉴于此，为了更好的从文本中提取信息，这里就引出了下面要描述的一种词位置矩阵——Position Embedding方案。</p>
<p>将Position Embeding机制发扬光大的，正是谷歌《All you need is attention》这篇论文。其原理在于通过word embedding嵌入层进行词组look up操作后，将所生产的张量E进行进一步特征提取。张量E的宽度为d维，则在张量E的第I维则通过以下式子进行转换计算。同时，式中pos表示张量E的每一个列元素。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-8dafcec5776c4d9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 POSemb图"></p>
<p>Position Embeding一般放置于词嵌入层后使用，在Transformer模型中，position embedding替代了传统RNN与cnn，直接通过记录词位置，作为文本上下文信息表征结构。而2018年谷歌Bert模型利用此机制对文本特征提取这一块进行改善，其将word embedding，句子层面嵌入向量和POS EMB三者相加进行结合训练。由此可见，POS EMB机制的有效性。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-1e08ced306766285.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2 BERT图"></p>
<p>我在此基于keras复现此机制，详情<a href="https://github.com/yanhan19940405/Self_Attention/blob/master/pos_emb.py" target="_blank" rel="external">请点击</a>查看。</p>
<p><strong>二．语序对于序列生成的影响与优化</strong></p>
<p>首先先提一下语言模型的概念吧。设一段文本序列长为n，其表征为x={x1,x2,x3,x4,……,xn}.则最终生成的该序列x概率可用条件概率表示: P=P(x1)P(x2|x1)P(x3|x2x1)P(x4|x3x2x1)……P(Xn|x1,x2,x3,x4,…,xn-1).因此在序列生成情况过程中，其条件概率分布及蕴含了文本上下文信息，从而可以看出语序对于文本而言重要性不言而喻。</p>
<p>在文本序列生成场景中，最常见的莫过于文本摘要，机器翻译，生成式对话，自动对联等应用场景。其原理基于seq2seq结构模型进行训练，通过Encoder将原始文本序列进行特征提取，转换为数字编码，生成隐藏状态向量state。之后与decoder端T0时刻目标文本序列构成的张量进行合并，从而输出T0+1时刻的目标文本序列。而在预测阶段的文本序列生成时，由于生成的文本是根据概率来分布，组合情况有多种，因此确认最优效的序列并进行输出便有了重要意义。在此阶段，我们可以使用集束搜索进行优化(Beem Search)。</p>
<p>BeemSearch原理大致如下：1.我们假设在Decoder端要生成序列A,B,C,D.若我们设置Beem search的词表大小为2，则预测阶段先生成词A作为序列开头位置。2.当后续要输入第二个词时，此时将会计算P(AB),P(AC),P(AD),P(AA)四种序列语言模型的概率大小。3.从中选出最大的概率P(AB)=P(A)P(B|A)。从而选择AB序列输出。4.后续又从词B开始，按照上述流程依次输出，直到输出整个序列ABCD为止。</p>
<p>BeemSearch实现很简单了，在decoder输出概率后，对其进行相应生成序列概率计算，即可完成整个序列生成步骤，此处不再举代码例子。</p>
<p>就到此为止吧！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/18/自然语言处理Attention机制综述与基于keras复现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/自然语言处理Attention机制综述与基于keras复现/" itemprop="url">自然语言处理Attention机制综述与基于keras复现</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T01:47:16+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在自然语言领域中，Attention机制借鉴于人脑得注意力机制流程，从而选择输入文本序列中关键信息进行处理，从而减少语言模型中带有序列信息得重点词与平常词没有区分得前提下，所带来的训练误差。</p>
<p><strong>1.Attention机制的种类</strong></p>
<p>从宏观上来讲，目前Attention机制主要分为聚焦式（focus）注意力机制和显著性(saliency-based)注意力机制。其中聚焦式注意力机制概念为依托于输入到神经网络的数据，按照从前往后的顺序，将数值较大的重要权重主动的、依赖预定任务的聚焦于某一对象上，从而将该对象与其他对象进行区分。而显著性注意力机制主要是神经网络中，从往前的一种反馈机制导致的被动注意力机制。这种注意力机制不受主动干预影响，只是一种神经网络为了更好的拟合数据，从而出现的被动反馈的调节机制而已。在LSTM与GRU中的门控机制可以近似看作这种注意力机制。最后，在自然语言处理过程中的注意力机制都属于focus一类。</p>
<p><strong>2.Attention机制运行过程</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-e0e8e5564e13fb72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 attention机制原理"></p>
<p>Attention机制运行过程如上图所示。其大致步骤如下：</p>
<p>1.输入文本序列，并将文本序列表征为类似于key-value这样的键值对<br>。<br>2.输入query句向量，用keyi与query句向量计算文本相似度。</p>
<p>3.进行softmax运算得出注意力权值ai=softmax(F(keyiquery))。此处ai可以理解为文本序列query中，第i（列）个信息受到的关注程度。</p>
<p>4.将value序列输入到网络，与ai 进行加权平均计算，从而得到序列编码值Attention。其中，在第三步文本相似度计算时，主要有如下几种方式:</p>
<p>1.加性注意力机制，计算如图所示，W和Wq分别为训练的超参数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-e4bbc565d941d2dc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 公式"></p>
<p>2.点积注意力机制，相似度计算如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-8ffec7d806b312ae.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2 公式"></p>
<p>3.缩放点积注意力机制·，其中d为词向量宽度，为了减少内存消耗，此处除以d进行缩放，减小计算量。相似度计算如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-c2eab6ed84bded8b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图3 公式"></p>
<p>4.余弦相似度注意力机制，相似度计算如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-6b45abd2fcb2e979.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图4 公式"></p>
<p>最后，上述注意力机制总体称为软性注意力机制(soft attention).其将文本序列x看作由key-value键值对构成。在此基础上，其通过与Query进行相似度计算从而对文本序列x进行软寻址。这也是被称为此机制的原因。</p>
<p><strong>3.Attention机制细节</strong><br><strong><strong>3.1外部注意力机制</strong></strong></p>
<p>当key=value=x，query=y时，attention机制计算流程与上述一致。此时注意力机制称为外部注意力机制。此时，softmax计算方式也进行了更改，计算公式如下，其他attention流程与上述图一致：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-cf131c7592e11594.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图5 公式"></p>
<p><strong><strong>3.2.自注意力机制(self-attention)</strong></strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-eed66941cf87b322.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图6 点乘缩放注意力机制"></p>
<p>自注意力机制是点乘缩放注意力机制变种，其结构如上图所示.当key=value=query=x时,分别对Q,K,V矩阵进行线性变换操作，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-08e965382c1aa442.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图7 公式"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-dffb58851d62ec8c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图8 公式"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-d1b58fcae80b0a96.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图9 公式"></p>
<p>其中，WQ,WK,WV为待训练的参数。其他后续流程与上述点乘缩放注意力机制一样，此处不再说明。</p>
<p><strong><strong>3.3.多头（muti-head）注意力机制</strong></strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-8b15dc048c74d8a7.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图10 多头注意力机制"></p>
<p>如上图所示，当进行了h次self attention之后，对每一次经过self attention得到的att值进行加性连接，运算公式如下所示，其中WO为待训练参数：</p>
<p>MultiHead(Q,K,V ) = Concat(head1,…,headh).WO</p>
<p><strong><strong>3.4硬注意力机制(Hard attention)</strong></strong></p>
<p>此类注意力机制与软性注意力机制的区别在于如下几方面：软性注意力机制输入的是定长的文本序列，同时其通过对文本序列中所有信息进行权值处理，从而进行编码。而硬性注意力机制只选择定长文本中，选择分布概率（语言模型）最大的信息作为输入，同时其用欠采样或者随机采样的方式进行信息筛选。最终得到长度不一致的序列进行训练。此处将会有更大问题，无法使用神经网络模型训练，因此一般都使用软性注意力机制进行训练。</p>
<p><strong>4.基于keras的self-attention机制复现</strong></p>
<p>我在此复现了下self-attention，其源码请<a href="https://github.com/yanhan19940405/Self_Attention" target="_blank" rel="external">点击</a>到git仓库查阅。</p>
<p>其输入：上一层的输出张量。</p>
<p>输出：一个三维张量，维度为（samples，maxlen，attention）,第一个维度含义代表句子数目，maxlen维度代表每一个句子长度，attention维度代表每个句子中每个分词构成的attention编码值个数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/12/模型融合方案Stacking原理与实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/12/模型融合方案Stacking原理与实现/" itemprop="url">模型融合方案Stacking原理与实现</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-12T21:46:03+08:00">
                2019-03-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>机器学习是基于统计学原理的一门技术。今天，我将给各位献上stacking模型融合方案原理简介与实现。首先，在此介绍几个前置概念。</p>
<p><strong>1.数据K折交叉验证(K-FOLD)</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-88674ef73823aa37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 数据K折交叉"></p>
<p>如上图所示，我们在获得一个完整数据集后，将数据分为K份（图中设置K为5），每次遍历数据取其中1份作为测试集，其余K-1份作为训练集。同时，对于任意一个基础机器学习模型而言，将会重复K次对数据进行处理的过程，这即为k-fold机制。</p>
<p><strong>2.基模型</strong><br>Stacking中的对数据K折交叉验证处理过程中，用到的机器学习模型。</p>
<p><strong>3.stacking原理</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-42aecde107ac6f39.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2 Stacking原理图"></p>
<p>Stacking原理如上图所示。下面我将结合具体数据维度，来对该机制进行说明。<br>假设我得到训练集Train_data，维度为：m<em>n,同时测试集Test_data维度为a</em>（n-1）。同时假设使用Train_data中所有特征进行训练，则从中取出一列label列表，其长度为m，此时Train_data维度变为(m*(n-1))。之后，设置K个机器学习模型作为基模型（本案例中所设置数字为5）。之后，我们分别取这K个模型中，每一个模型进行训练集上K折交叉处理。在一次迭代中，将数据的第K份作为测试集，其他K-1份作为训练集，从而一次迭代生成维度为(m/k,(n-1)/k)的矩阵。将每一次生成的矩阵拼接起来，则对于一个模型而言，完整K次遍历数据后，将生成维度为（m,n-1)的矩阵。则K个模型而言，我们将这些矩阵横向拼接起来，得到最终超特征矩阵new_train，维度为(m,kn-k)。</p>
<p>除此外，对于测试集Test_data而言，我们也在基模型每次遍历测试集并训练出模型时，对测试集进行预测。每个模型每次遍历得到的测试数据集矩阵维度为(a,n-1),则<br>k次遍历后，最终的到的矩阵维度为(a,kn-k).此时为了保证维度统一，我们对该矩阵进行按行求取均值，将其维度变为(a,n-1)。对于K个模型而言，最终经过上述处理过程后，得到新的测试集上的超特征矩阵new_test，其维度为(a,kn-k)。</p>
<p>上述过程即为stacking第一层处理过程。之后第二层处理过程中，我们将new_train作为训练集，将第一层得到的label集合作为标签进行训练，得到最终训练出的模型，从而对new_test进行预测即可。</p>
<p><strong>4.stacking扩展</strong></p>
<p>在kaggle的比赛上，有的团队在经过第一层处理后得到的new_train矩阵基础上，与原始的train_data矩阵进行横向拼接。同时，对new_test也做此处理。从而选取K个机器学习模型，继续对新数据进行K-fold处理。后续操作与上述同理。但一般经过三层即可结束了。</p>
<p><strong>5.stacking代码实践</strong></p>
<p>代码已经上传至git仓库，请<a href="https://github.com/yanhan19940405/Data_machine_learning/blob/master/stacking_class.py" target="_blank" rel="external">点击查看</a></p>
<p>我用此机制跑了跑某比赛，在没有调参的情况下，与第一名指标相差不是很大，还是较为满意的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/10738320-156f4284c27c6078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图3 跑分图"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/文本数据挖掘流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/文本数据挖掘流程/" itemprop="url">文本数据挖掘流程</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-21T11:02:24+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要记录文本数据挖掘大致流程，用以梳理该部分结构知识。<br><strong>1.数据获取</strong><br>对于文本数据而言，在公司项目中大部分数据可以通过购买方式获取数据。但常见的方式即为通过网络爬虫获取，详细请参考我后续爬虫技术教程。该种方式获取数据原理是通过程序自动遍历固定网址，将整个页面HTML结构中，不同页面DOM节点文本信息收集整理，然后缓存到本地数据库中。数据集具有结构不唯一，含有HTML标签，特殊符号等不利于训练的数据，需要后期进一步处理。<br><strong>2.数据清洗与数据预处理</strong><br><strong><strong>2.1文本数据清洗</strong></strong><br>在获得的文本数据集，也叫语料库，里面含有许多无关的HTML标签，前端JS代码段，注释，无关特殊符号等增加训练噪声的数据，因此我们需要对数据进行处理，筛选掉这些无关数据项，技术上一般使用python正则表达式处理即可。最后，对于文本数据中的标点符号，可以剔除掉，用空格来代替。<br><strong><strong>2.2结构化数据</strong></strong><br>通过上述步骤处理后，我们就有了训练语料库。但对于文本数据挖掘而言，运用机器学习模型或者深度学习模型来训练的话还远远不够。对于监督学习，无监督学习，半监督学习而言，监督学习在文本数据瓦解运用上来看目前效果较好，也是最合理的方式。因此我们需要对文本进行打标签（label），这对于回归与分类问题而言具有重要意义。最后，我们要将不同网页的文本数据单独保存为一个文档，然后添加这个文档数据的标签，整个数据集就由多个带标签的文档构成，也叫作文档库。于是数据就有了词汇——》文档——》文档库这么一个简单结构，有利于构成训练矩阵。最后对于部分结构数据中存在的NAN数值项，在大文本训练情况下将这类数据删除即可，数据量小的可以用数值0代替。<br><strong><strong>2.3 分词</strong></strong><br>中文分词原因与处理过程请见我另一篇文本主题分类文章，这里不再详述。<br><strong>3.文本挖掘数值特征工程</strong><br>首先，文本数据集内部全是各种str类型的文本分词数据，这类结构无法直接带入到计算机中进行计算。因此我们需要将数据进一步处理成能够让计算机计算，标注的一种矩阵结构。目前做法一般分两种。<br><strong><strong>3.1词袋模型(BOW)</strong></strong><br>此类方式通过忽略分词语义特征与前后顺序，直接将所有分词放在一个统一的文档集合中，统计每一个分词出现的次数，最后得出一个矩阵。矩阵的列代表每一个分词的出现次数，矩阵的行代表文档数量，在这篇文档出现了的分词次数为m，就在那个位置标注为m即可，其他没出现的文档位置直接标注0，这类矩阵即为词频矩阵，其横向维度即为分词数目，长度会很长，因此对于低频词影响没法忽略掉，需要进一步特征筛选。最后，这类矩阵可以直接带入模型中训练。而目前大部分使用的TF-IDF算法效果较好，其引入了逆文档频率IDF，在词频概率TF基础上计算的词频矩阵能更好的契合主题分类模型。<br><strong><strong>3.2词向量模型</strong></strong><br>这种方式通过one-hot编码将分词序列化。举个例子，我有段文本“今天天气好晴朗”经过分词处理后的结构变为“今天”，“天气”，“好”，”晴朗“。那么通过one-hot编码处理后的每一个分词向量即为：<br>“今天”=[1,0,0,0],”天气”=[0,1,0,0],”好”=[0,0,1,0],”晴朗”=[0,0,0,1],最后这个句子（文档）的词向量模型即为[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]].这样不同的文档词向量矩阵组合在一起就构成了一类高维度的文档集合词向量，矩阵的列代表词表大小即为分词个数，行代表文档个数。但考虑到对于分词而言，每一个词向量列元素只有其所在位置标1，而其他位置全部标0，这会造成数据分布不均问题。所以上述简单处理方式外，还有word2vec,doc2vec,fasttext模型进一步优化处理（fasttext,word2vec模型分类过程后续详谈)。有一种基于哈夫曼树的正负训练方式值得关注，其将正负表示词语语序排布问题，在树上正负分别表示左右处理方向，对于语言相似性判断有极大作用。最后，词向量模型高度抽象，无法直接带入统计机器学习模型训练，所以最好与深度学习模型进行嵌入训练，效果最佳。<br><strong><strong>3.3特征选取</strong></strong><br>经过编码处理后的矩阵词库大小依然太大，全部带入模型训练的话，拟合起来可能速度太慢。因此需要选择数据量合适，表征能力强，区分度大，经过编码处理后的分词作为训练数据集的特征。因此具体问题就回到了传统数据挖掘数据特征选取问题，一般采用卡方检验，信息增益，随机森林算法进行特征提取即可。<br><strong>4.训练模型和调参</strong><br>这一步就需要看你选择的模型来操作了，从大体上来讲，模型需要对数据进行拟合，要达到相应精度要求，除了确保数据质量高，特征选取合适外，还需要合理的调参设置。为了增强具体模型预测的泛化性，设置合理的L1,L2正则化项也是需要考虑的。此部分就不展开说了，后续结合实例再谈。<br>最后整个文本数据挖掘流程大致如此了，后续可以在这样处理框架下细化操作即可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/19/基于TFIDF-XGBoost文本主题分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/19/基于TFIDF-XGBoost文本主题分类/" itemprop="url">基于TFIDF+XGBoost文本主题分类</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-19T19:25:30+08:00">
                2018-07-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在丰富的自然语言处理领域中，文本分类是最基本的应用场景，因此使用一套合适的方案对于文本分类上的运用具有积极的效果。本文主要介绍基于词频的TF-IDF的特征提取，加上XGBoost的文本多分类方方案。<br><strong>1.数据预处理</strong><br>首先，文本数据源来自于网络爬取数据，所有标签都通过人工标注的方式产生，具有数据质量不高，结构复杂的问题。其结构为每一行代表一个新闻训练实例，其中context中表示爬去的新闻具体文本内容，tittle表示新闻标题，theme表示每一个新闻训练实例的标签(label)。从中可以看出数据中含有网络爬虫包含的HTML标签，部分数据项为空，包含无关标点符号(比如逗号，顿号，句号等),也包含了数字等等，同时也包含文本主题分类无关的高频词汇（如的，得等）。这些内容增加了利用监督学习训练模型的的噪声干扰。因此对于数据我这里做了基本预处理，引入Python正则表达式库，将HTML标签，文本标点符号，数字内容过滤掉，然后引入中文停词表(可以上网络下载)，也将数据集中的文本在中文文本分词阶段，过滤掉在停词表中的词汇。<br><strong>2.中文文本分词</strong><br>对于自然语言领域而言，中英文数据集有个显著特点，就是中文词汇之间没有明显的分隔符，而英文之间却有自然的空格符来分隔，因此数据需要使用分词工具来将一大段文本分割为不同的单词词汇，用以转为训练矩阵进行主题分类。这里使用的是中文jieba分词工具，将每一段文本处理成分词结构，之后存储于一个txt文件中，分词处理的代码和处理后的数据如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/10738320-5cccca5058eb4b22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/490" alt="图1 分词代码"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-3ab8312a716db8fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="图2 分词处理后的数据格式"><br><strong>3.文本特征提取</strong><br>经过如上步骤的文本数据处理过程之后，我们需要将文本文件转化为数值向量以便于能够带入机器学习分类器进行分类，本文此处使用的是TF-IDF算法进行处理。<br><strong><strong>3.1 TF-IDF算法原理</strong></strong><br>该算法通过统计字词分布频率用以评估一个字词对于一个文本数据集的重要程度。字词的重要性是指如果它在文本中出现的次数越多，那么其所占概率分布相比于其他词汇更高，则表明该词汇对这个文本而言越重要。TF-IDF的算法思想就在于如果一个分词相对于一个文档出现频率（TF)越高,则表示这个分词对于这个文档而言很重要。如果一个文档集合中（多个文档构成的集合），该类分词出现的文档数目越少，则越能将此类文档与其他文档区分开，而表征此类文档出现的状态参数即为IDF（逆文档频率）值大小。最后将二者频率相乘，即可得到文本词频矩阵，我们也就完成了文本向量化过程。<br><strong><strong>3.2TF-IDF算法过程</strong></strong><br>1.计算文本中每一个分词频率（TF）<br><img src="https://upload-images.jianshu.io/upload_images/10738320-2069b690c41cb14b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/549" alt="图3 分词频率计算"><br>2.计算每一个分词的逆文档频率值(IDF)<br><img src="https://upload-images.jianshu.io/upload_images/10738320-68c3df705f7bdad7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/674" alt="图4 分词频率计算"><br>3.计算该类分词文本词频的值（TF-IDF），计算公式即为：TF-IDF=TF<em>IDF<br>4.将计算所得所有分词词频值构成一个矩阵<br><strong><strong>3.3IF-TDF算法缺陷</strong></strong><br>基于TF-IDF的方法是通过统计的思想来实现文本数据向数值型数据转化的。在中文文本语义中，一般开头第一二句话表示该文本主题思想。而这类方法有个缺点，即为通过频率分布可能无法得到表示某一类文本中心思想的主题句段，字词的特征，因此在文本情感分类，主题分类方面存在误差，也可以说这也是基于词频的文本处理方案的通病。<br><strong>四．文本分类</strong><br>经过以上步骤处理过的文本数据集就转化为了词频矩阵。这类矩阵维度适中，可以直接带入统计机器学习库进行主题分类处理，本文在之前比赛经验中，决定采取统计机器学习中对于高精度要求具有普适性的XGBoost进行建模应用，以达到分类目的。<br><strong><strong>4.1 XGBoost算法前置理论</strong></strong><br><strong>**</strong>4.1.1 决策树模型与Boosting算法<strong>**</strong><br>决策树作为一类基础的二叉树模型，每一个决策结果只有“是”或者“否”，其简单，易分类的特点使其广泛用于机器学习分类与回归的基础场景，这里一般采用CART生成算法，为了减少过拟合现象，对于生成的树会进行剪枝操作（该部分详细理论请参阅李航《统计学方法》第五章）。<br>Boosting算法，即提升方法，该算法思想通过将不同的弱分类器线性叠加，同时赋予相应权重，以对于数据进行拟合，从而达到相关分类或者回归效果。其通过拟合下面的假设函数（如下图所示）进行建模。其中w为模型基函数（即为弱分类器），W（m）为每一个弱分类器权值。<br><img src="https://upload-images.jianshu.io/upload_images/10738320-1bd133646adc31b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/301" alt="图5 假设函数"><br>一般Boosting算法建模过程如下图所示，来源于李航《统计学方法》：<br><img src="https://upload-images.jianshu.io/upload_images/10738320-850408ef6d011e04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/693" alt="图6 boosting建模过程"><br>其思路即为不同弱分类器线性加法，通过添加不同的弱分类器进行训练，得到损失函数最小值后即停止模型计算过程。<br><strong>**</strong>4.1.2 Gradient boosting(GB)算法<strong>**</strong><br>Gradient Boosting是一种Boosting的方法，其代价函数是常见的拟合程度+正则化项结构，损失函数值取得最小值则代表该模型效果最好。它在数据建模过程中，每一次建模会对于通过代入预先设定好的不同弱分类器和弱分类器数目进行建模。其在模型损失函数的梯度下降方向进行探索，在找到沿着梯度方向下降的时候，此时找到了模型损失函数瞬时最小值，会记录参数进行建模。之后其会进一步沿着梯度下降方向进行相同操作，最后当梯度值变化不大或者相等时停止整个过程。除此外GB算法中弱分类器可以更换为任意机器学习基础模型。<br><strong>**</strong>4.1.3 Gradient boosting Decision Tree(GBDT)算法<strong>**</strong><br>顾名思义，该算法即为GB算法与决策树相结合，其将多个决策树模型作为弱分类器，代入到GB算法作为框架拟合数据集的算法模型。其中CART决策树可以用如下假设函数来表示：<br><img src="https://upload-images.jianshu.io/upload_images/10738320-5ea34c3d7606084f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/247" alt="图7 GBDT算法假设函数"><br>其中Rj表示假设空间，γ每次决策树分叉的返回值，I()表示指示函数，在空号中条件成立情况下为1，否则为0.其中的参数J表征树的深度。<br>而GBDT算法流程大致如下图所示：（图片来源于《The Elements of Statistical Learning》一文）<br><img src="https://upload-images.jianshu.io/upload_images/10738320-85553775645f0a4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/478" alt="图8 GBDT算法流程"><br>其中可知该算法关键在于通过不断迭代损失函数值，从而不断拟合下面的式子（如下图所示），从而得到最优解。（注意该式子有的材料将其称作残差计算式，但该算法核心还是在于梯度更新最优解）<br><img src="https://upload-images.jianshu.io/upload_images/10738320-e0e2d46222dee3dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/294" alt="图9 损失函数迭代式"><br><strong>**</strong>4.1.4 XGBoost算法<strong>**</strong><br>Xgboost是基于GB算法框架来实现的，其弱学习器除了可以是CART决策树也可以是线性分类器。通过参阅paper<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="external">《XGBoost: A Scalable Tree Boosting System》</a>，其跟CART决策树的GB算法大致有如下区别:<br>(1). 如下图所示，xgboost在损失函数中的拟合程度，加上了正则化项，当学习器为CART决策树时，正则化项fk与树的参数相关。<br><img src="https://upload-images.jianshu.io/upload_images/10738320-adf87085cf8bd4df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/390" alt="图10 XGBoost损失函数"><br>(2).如下面两幅图所示， GB中使用使用损失函数对f(x)的一阶导数计算出梯度最优解，来迭代损失函数从而学习生成f(x)，而XGBoost算法在迭代过程中是将一阶导数和二阶导数结合起来进行f(x)更新从而达到快速优化每次模型训练速度的目的。如下图所示，第t次损失函数更新计算式。<br><img src="https://upload-images.jianshu.io/upload_images/10738320-991097111d4f25ce.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/332" alt="图11 XGBoost损失函数"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-e1f41b2139246ce4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/402" alt="图12 XGBoost损失函数泰勒展开"><br>(3). 如下四副图所示，在文中等式4表示XGBoost模型损失函数，其中XGBoost利用5式计算叶节点最优权重值和利用式子6计算叶子节点最优值。CART回归树中寻找树每一层根节点的最佳切分点的衡量标准是最小化均方差，而在XGBoost中的切分点的标准是树的LR和LF集合并集最大化，计算式如式子7所示 ，式子一般用来计算取得最佳切分点位置，式子中参数都在损失函数（式子4）每次取最优解后迭代更新，确定最优切分点位置。<br><img src="https://upload-images.jianshu.io/upload_images/10738320-cd47af1a7190f545.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/408" alt="图13 式子4"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-3094421045b44c61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/422" alt="图14 式子5"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-e408999a07bb7871.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/384" alt="图15 式子6"><br><img src="https://upload-images.jianshu.io/upload_images/10738320-37f6d47ea8b20330.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/435" alt="图16 式子7"><br><strong>**</strong>4.1.5XGBoost算法实现<strong>**</strong><br>代码如下图所示，此处基于XGBoost的sklearn入口实现算法应用，其中在数据切分后，先来者5轮交叉验证进行模型参数选择，之后利用选取的参数模型进行建模训练，最终得出相关数据报告即可，这部分关键代码截图如下所示：<br><img src="https://upload-images.jianshu.io/upload_images/10738320-ccd4c0c19e43052d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="图17 算法代码"><br><em>*五．文本主题分类结果</em></em><br>最终我们选取了learning—rate=0.1的模型进行建模，最终结果如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/10738320-392e4a41f943e1a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="图18 数据分析报告"><br>模型精度达到了74%，远远胜过于贝叶斯的63%与logistic模型的69%（这两类模型作者之前跑过作为对比），的确XGBoost方法对于高精度应用场景具有更好的效果。最后在输出模型标签分布图，即可看到具体标签重要程度，可以进行建模的特征选取。如下图所示：<br>（注意：此处作者训练服务器python库matpolib中文乱码，就不再处理，详情请自己添加中文规则进入matpolib源码中设置即可）<br><img src="https://upload-images.jianshu.io/upload_images/10738320-c6753e9cf4d7df86.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="图19 特征重要性分布图"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/08/机器学习算法实践之决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/08/机器学习算法实践之决策树/" itemprop="url">机器学习算法实践之决策树</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-08T08:17:15+08:00">
                2018-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>决策树是一种基本的分类与回归方法。决策树模型呈现树形结构，在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是if-then规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布，其主要优点是模型具有可读性，分类速度快。在进行数据学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时对新的数据利用决策树模型进行分类。决策树模型学习过程主要包含三个步骤：特征选择，决策树生成，决策树修剪。我们将在后面逐步探讨这些内容。<br><strong>1.决策树模型理论</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/28/区块链技术体系初步/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="VKYH">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VKYH-2017-8-17">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/28/区块链技术体系初步/" itemprop="url">区块链技术体系初步</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-28T11:01:59+08:00">
                2018-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>1.背景</strong><br>在传统互联网的TCP/IP协议下，我们进入了信息爆炸时代。在当前情况下，信息快速成型，低成本传输与大量信息共享得到了实现。目前的互联网金融模式是以中心化记账形式来进行等价值货币转移而运行的，比如A在互联网上转账给B，A需要通过网上发送请求给中心化机构承担者，通过在中心机构记账的方式，将信息转化为价值，再由中心机构将等价值的货币转移至B的账户上。这种情况需要政府或者企业利用信用为中心化机构承担者做出保障，但由于传统信息互联网部分个体篡改局部数据情况无法及时发现甚至无法掌握相应情况，因此这种结构依然存在安全性问题。<br> 因此，我们如何实现传统网络传递价值？这就是近几年火起来的新技术——区块链。2008年中本聪在互联网上一个讨论信息加密的邮件组中发表了一篇文章，文章名叫《Bitcoin: A Peer-to-Peer Electronic Cash System》，勾画了比特币系统的基本框架。 2009年他为该系统建立了一个开放源代码项目 (open source project)，正式宣告了比特币的诞生。区块链技术刚开始是为了支持比特币生成与交易而产生的底层技术，其目的是通过建立去中心化的信任，从而不再依托于单个中心结构记账，从而通过P2P网络直接进行价值交换，从而使得实现传统信息交换互联网变为价值交换互联网成了可能。后续这门技术得到了进一步提炼发展，便从比特币中提取出来，成了今天的区块链技术。<br> <strong>2.区块链的初步以及几个名词解释</strong><br> <strong><strong>2.1 什么是信用？</strong></strong><br> 信用，是指依附在人之间、单位之间和商品交易之间形成的一种相互信任的生产关系和社会关系。如果我们用数据m量化描述这个概念，那么当m的值越高，则描述个体的信用程度越大，人们就更容易相信该个体发布的交易与账户信息。目前的中心化交易体系就是如此，国家建立法制的社会信用体系，利用国家公信力与强有力的法律为银行做保证，从而提升市场对于银行的信用程度，通过银行这一中心化的组织来进行经济行为有效监管与控制。然后大型互联网企业则利用自身平台影响力，不断提升自己平台影响力，通过自身理念与遵守相关法律法规增加用户对其信用基础，从而让用户能够安心使用平台所提供的网络价值交换服务。<br> <strong><strong>2.2 什么是价值转移？</strong></strong><br> 价值转移，是指具有同等购买力的货币进行流通。在当前金融体系下，价值转移行为需要由第三方进行价值记账，价值交换等行为的操作，控制，才能实现价值在不同个体间的转移。<br> <strong><strong>2.3 什么是P2P网络？</strong></strong><br>P2P网络，即对等网络（Peer-to-peer networking），网络参与者共享他们自己的硬件资源（处理能力，存储容量，网络连接，打印机等）的一部分，通过网络提供服务和内容，并且可以由其他对等方直接访问而无需中间实体。 该网络的参与者既是资源，服务和内容提供者（服务器），也是资源，服务和内容获取者。在P2P网络环境中，多台彼此连接的计算机处于相同的位置。 一台电脑可以作为服务器并为网络设置共享资源使用其他计算机，也可作为工作站，网络一般不依赖于专用的集中式服务器，也没有专门的工作站。 网络中的每台计算机既用作对网络服务的请求，也用于响应其他计算机提供资源，服务和内容的请求。<br> <strong><strong>2.4 区块链的定义</strong></strong><br> 区块链技术是指一种去中心化的分布式共享记账存储技术，本质上是数据库技术，是一连串使用密码学方法产生相关联的数据块，每一个数据块即为区块，其包含了一段时间内全网交易信息和用于验证其有效性的信息，每一个区块通过指针连接，上一个区块可以创建下一个区块，整体呈现链式排布。<br>如下图所示，即为传统中心化交易体系，这类体系下中心企业信用度指数要高，同时对于现有以信息传递的互联网体系下，无法实现在线价值转移必须经过第三方平台转移之后才能实现此功能。<br><img src="http://upload-images.jianshu.io/upload_images/10738320-3a28b8f7e0b649bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1 中心化交易体系"></p>
<p>这类体系存在如下问题：1.黑客可以修改转账信息，更改数据后，如果无有效交易检验方法，则在现有的信息网络情况下，这种情况不容易被发现，然后矫正。2.中心化体系下，必须对于中心单位给予强有力的信用保证，但对于网上支付平台下，大部分企业自监自查，一旦中心单位出现数据丢失等不可控困难，这种情况存在很大经济波动风险。3.传统交易体系下，网上支付无法直接在线进行价值交换，使用不是很方便。为了解决上述问题，区块链技术便横空出世了。<br><strong><strong>2.5 区块链系统交易过程</strong></strong><br>如下图所示，在区块链技术体系下，一个区块链系统有很多节点构成，每一个参与的节点都能够记账，即更新数据库信息。区块一利用私钥签名创建交易后，将信息发往全网进行广播，其交易记录通过P2P网络向后传播，然后经过系统共识机制对交易进行验证，确定交易信息有效性后，将验证结果与上一个节点交易记录一起再次利用P2P网络传递给下去，最后各个记账节点依托于验证结果将交易记录写入账本，更新区块存储信息，从而实现了去中心化的分布式记账。<br>而共识机制是用来确定分布式系统节点中的账本记录节点的一种机制，其中目前最常用的是工作量证明机制——各个节点通过区块接入系统，通过本地计算能力高低来确认记账与否的能力。计算能力越高的节点，则越能成为账本记录节点，就能将自己节点的交易信息形成一个新的区块加入链中。一旦加入链中，如果想要修改一个区块链中某个区块交易信息，就必须完成该区块与后一个区块的工作量，从而让更改数据成为不可能。<br><img src="http://upload-images.jianshu.io/upload_images/10738320-2614154946b819f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2 区块链交易过程"><br>鉴于此，实现区块链技术的关键因素也就出来了，即:1.由于每个节点可以自由加入或退出系统，则形成有效的盘P2P动态网络是关键因素。2.形成时间有序排列的切不可更改的系统主体交易账本（由于个体交易账本篡改不会影响后续节点，真实交易记录会在多个节点存在，因此更改局部节点交易账本一般不会对整体系统产生作用）。3.系统含有有效的统一采用的共识机制模型。<br><strong>3.区块链技术理论详解</strong><br><strong><strong>3.1 区块与区块链结构</strong></strong><br><img src="http://upload-images.jianshu.io/upload_images/10738320-9f17b969edf2869b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图3 区块结构"><br>区块结构如上图所示，每一个区块包含前一个区块ID，交易记录信息，本区块ID等信息。其中交易记录也被称作账本，其结构有两种，一种是采用链式Hash结构防篡改，一串数据hash区块组成链，每个区块账本包含了最近交易集合的hash值，一旦某个部分数据更改，那么hash校验就会出现许多新的结果，从而实现欺骗行为的甄别。另一种是采用公钥密码机制标识的资产所有者，从而利用公钥做为标识，仅仅只有相应私钥的用户才能拥有转移价值行为操作基础。<br><img src="http://upload-images.jianshu.io/upload_images/10738320-74c585d2ac9aadc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图4 区块链结构"><br>如上图所示，区块链是由多个区块连接起来的一种链式结构，区块链之间彼此用指针相互对应关联起来，如图3.1-2，由于每一个区块都可以创建一个新的区块，故需要一种共识机制，也是目前应用最广泛的工作量证明机制，通过比拼不同区块也就是节点的计算能力，算力越高，则就越能获得合法的记账权，从而挂载入区块链，同时产生下一个区块。算力不高的区块就无法挂载入链中，其原理类似于大自然中动物群体生殖权争夺。而新区块的ID不一定是前一个区块加1，此处仅仅作为合理性展示才如此画图。除此外，链越长，信用指数越高，越容易受到人们认可，除此外区块链采用高冗余度的存储系统来进行数据存储，在运用工作量证明机制后，也带来高能耗等缺点，此处我们后面再谈。<br><strong><strong>3.2 区块链的模型架构</strong></strong><br>首先提两个概念，公有区块链与私有区块链。公有区块链是指全世界任何人都有权限，可随时随地读取的、任何人都能发送交易且交易能获得有效确认的、任何人都能参与其中共识过程的区块链，对用户访问区块链系统不做权限限制的区块链，链越长信用度就越高，这也是公有链越能获得大多数人认可的重要原因。而私有区块链通过严格的权限限制供特定人访问运行，安全度得不到保障。<br>如下图所示，区块链模型结构类似于TCP/IP模型结构一样，由图中所示六层组成，从技术最底层向上排列分别是数据层、网络层、共识层、激励层、合约层、应用层。其中数据层，封装了底层数据区块的链式结构和相关的非对称数据加密技术和时间戳等技术；网络层，包含了分布式组网机制数据传播和验证机制，对于交易过程控制意义重大；共识层封装网络节点的共识机制算法模型，对于确立记账节点有重大作用；激励层则包含经济发行机制与分配机制，用于激励遵守规则参与记账的节点，惩罚违反规则的节点；合约层则封装各类算法，编程协议，这给了区块链技术编程基础；应用层则为各类区块链应用程序提供了运行环境。其中图中红色三层为区块链必须的层数，其他三层为非必需层数。<br><img src="http://upload-images.jianshu.io/upload_images/10738320-2d2455b54bad9a66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图5 区块链模型结构"><br>除此外，在私有链中，一般不含有激励层，可能由私有链使用者添加强制性因素进行记账节点分配。<br><strong><strong>3.3 区块链的共识机制模型</strong></strong><br>共识机制是区块链技术的核心，其主要起到了交易账本信息验证和确认的作用，同时其决定了哪个区块节点为共识记账节点。同时在拼接区块链时，在上一个区块具体挂载下一个区块时的，所需区块节点也由工作量证明机制来决定的。此处介绍两个运用广泛的共识算法模型。<br><strong>**</strong>3.3.1 工作量证明机制(POW)<strong>**</strong><br>如2.5节的图中所示，在一个稳定的，多个节点达成统一共识机制的系统中，其中一个节点作为证明者，提交已知的，难以计算但容易验证的结果，其他任何节点都能通过验证这个结果确信证明者为了求得结果已经大量完成了相应工作量的计算结果，这就是工作量证明机制。比如飞行员有了10000小时飞行时间，则我们都可以相信这个飞行员有高超的驾驶技术，工作量机制与之同理。工作量证明机制中，其利用SHA256 hash算法计算工作量。一个符合要求的区块hash值由n个前导邻，前导邻个数取决于网络的复杂度。要得到合理的区块hash需要大量的运算，运算速度取决于机器hash运算速度。一个节点拥有合理hash值是一个概率性事件，因此当节点拥有合理hash值时，需要运用大量资源进行计算，完成了相当的工作量，因此算力值越高的节点就越能成为记账节点，这也是挖矿时严重耗电的重要因素。<br>获得记账权概率为算力占总节点算力值的概率值，这也为POW系统的安全提供了很高的保障，毕竟要想攻破一个基于POW的区块链系统，没有一定的算力资源可没法攻破这个系统。<br><strong>**</strong>3.3.2 权益证明机制(POS)<strong>**</strong><br> 这是一个shy256算法替代品，避免大量计算工作量的算法模型。其通过对每一笔的交易销毁的币/天数，来实现证明者对某些数量的钱展示所有权。币天数代表一个特定的币，作为网络中权益代表量化单位，一旦有交易发生币天数就会被销毁且无法重复使用。在给定时间点内，存在有限币/天数是有限的，鉴于此在区块链中，持有更多数据货币的人就会有更多币天数，则类似于股份权益，占比更多的人分红更多。<br> POS安全性将不再靠工作量来保护，而是通过链的一体性来决定的其安全性的，即凡是处于POS中的人区别只是收益不同，但大家都有收益的前提是系统安全性，因此其中大股东更加注重链的安全性与完整性。<br><strong><strong>3.3 区块链的类型</strong></strong><br><strong>**</strong>3.3.1 公有链与私有链<strong>**</strong><br> 具体阐述请见3.2区块链模型架构。此处谈一谈公有链与私有链区别，公有链一般含有一个完整的代币系统，在激励层能够奖励遵守规则的记账节点，惩罚违反规则的记账节点，而私有链中节点是某个组织内部节点，注入权限就不会含有代币系统。公有链就类似于互联网，私有链就类似于局域网，当前大部分公司都在采用私有链技术。<br><strong>**</strong>3.3.2 联盟链<strong>**</strong><br>  其共识过程受到选择预先节点选择控制的区块链，在系统建立前就预先决定记账节点分布与其他节点状态。<br><strong>**</strong>3.3.3 许可链<strong>**</strong><br> 是指每个节点加入区块链需要取得许可的一种区块链种类。<br><strong>**</strong>3.3.4 混合链和复杂链<strong>**</strong><br>伴随着区块链技术发展，链中每个节点可以根据不同需要通过权限控制其功能，比如在同一个系统中，有的节点记账，有的节点可以查询整个链信息，有的节点只能查询部分信息，从而构成了一个复杂但高效的区块链网络。<br><strong>4.总论</strong><br>区块链初步理论体系记录到此结束了，但智能合约部分暂未记录，还在研究，在后续会进行更新完成。从目前收集资料上来看，区块链技术的确有对互联网进行变革的潜力，其价值转移互联网一旦成型，将会对目前的信息网络带来更多积极的影响，值得关注。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="VKYH" />
          <p class="site-author-name" itemprop="name">VKYH</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">VKYH</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  










  





  

  

  

  

  

  

</body>
</html>
